# -*- coding: utf-8 -*-
"""cropp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xbrwmIMk6Q73QPNTt7f30xoJudo9nd6i
"""

import os
import shutil
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import layers, models
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

tf.config.list_physical_devices('GPU')

from google.colab import files
uploaded = files.upload()

!unzip EuroSAT.zip

ls

!ls 2750

!mv "EuroSAT (1).zip" EuroSAT.zip

!rm -rf land_data

import os
import shutil

# Remove old folder if exists
!rm -rf land_3class

os.makedirs("land_3class/Agriculture", exist_ok=True)
os.makedirs("land_3class/Forest", exist_ok=True)
os.makedirs("land_3class/Urban", exist_ok=True)

# Agriculture classes
agri_classes = ["AnnualCrop", "PermanentCrop", "Pasture", "HerbaceousVegetation"]

# Urban classes
urban_classes = ["Residential", "Industrial", "Highway"]

# Copy Agriculture
for cls in agri_classes:
    for file in os.listdir(f"2750/{cls}"):
        shutil.copy(f"2750/{cls}/{file}", "land_3class/Agriculture")

# Copy Forest
for file in os.listdir("2750/Forest"):
    shutil.copy(f"2750/Forest/{file}", "land_3class/Forest")

# Copy Urban
for cls in urban_classes:
    for file in os.listdir(f"2750/{cls}"):
        shutil.copy(f"2750/{cls}/{file}", "land_3class/Urban")

print("3-Class Dataset Created Successfully!")

import random

def limit_images(folder, max_images=3000):
    for cls in os.listdir(folder):
        path = os.path.join(folder, cls)
        images = os.listdir(path)
        if len(images) > max_images:
            remove_images = random.sample(images, len(images)-max_images)
            for img in remove_images:
                os.remove(os.path.join(path, img))

limit_images("land_3class", 3000)
print("Dataset Balanced!")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

IMG_SIZE = 160   # slightly larger for better accuracy
BATCH_SIZE = 32

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True
)

train_data = datagen.flow_from_directory(
    "land_3class",
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    subset="training"
)

val_data = datagen.flow_from_directory(
    "land_3class",
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    subset="validation",
    shuffle=False
)

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import layers, models

base_model = MobileNetV2(
    input_shape=(IMG_SIZE, IMG_SIZE, 3),
    include_top=False,
    weights="imagenet"
)

base_model.trainable = False

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(3, activation="softmax")
])

model.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=5
)

from tensorflow.keras.optimizers import Adam

base_model.trainable = True

for layer in base_model.layers[:-30]:
    layer.trainable = False

model.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

fine_history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=3
)

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label="Train")
plt.plot(history.history['val_accuracy'], label="Validation")
plt.title("Training Accuracy")
plt.legend()
plt.show()

plt.plot(history.history['loss'], label="Train")
plt.plot(history.history['val_loss'], label="Validation")
plt.title("Training Loss")
plt.legend()
plt.show()

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

val_data.reset()
pred = model.predict(val_data)
pred_classes = np.argmax(pred, axis=1)

true_classes = val_data.classes
class_labels = list(val_data.class_indices.keys())

cm = confusion_matrix(true_classes, pred_classes)

plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt="d",
            xticklabels=class_labels,
            yticklabels=class_labels,
            cmap="Blues")
plt.title("Confusion Matrix")
plt.show()

import matplotlib.pyplot as plt

# Count predicted labels
import pandas as pd
predicted_labels = [class_labels[i] for i in pred_classes]
df = pd.DataFrame({"Predicted": predicted_labels})
counts = df["Predicted"].value_counts()

# Pie Chart
plt.figure(figsize=(6,6))
plt.pie(counts, labels=counts.index, autopct='%1.1f%%')
plt.title("Land Use Sustainability Distribution")
plt.show()

green_cover = counts["Agriculture"] + counts["Forest"]
urban_cover = counts["Urban"]

plt.figure(figsize=(6,5))
plt.bar(["Green Cover", "Urban"], [green_cover, urban_cover])
plt.title("Green Cover vs Urban Land")
plt.ylabel("Number of Images")
plt.show()

years = [2018, 2019, 2020, 2021, 2022]

green_trend = [
    green_cover + 300,
    green_cover + 200,
    green_cover + 100,
    green_cover + 50,
    green_cover
]

urban_trend = [
    urban_cover - 200,
    urban_cover - 100,
    urban_cover - 50,
    urban_cover - 20,
    urban_cover
]

plt.figure(figsize=(8,5))
plt.plot(years, green_trend, marker='o', label="Green Cover")
plt.plot(years, urban_trend, marker='o', label="Urban")
plt.title("Simulated Sustainability Trend")
plt.xlabel("Year")
plt.ylabel("Land Count")
plt.legend()
plt.show()

green_cover = counts["Agriculture"] + counts["Forest"]
urban_cover = counts["Urban"]

years = [2018, 2019, 2020, 2021, 2022]

green_trend = [
    green_cover + 300,
    green_cover + 200,
    green_cover + 100,
    green_cover + 50,
    green_cover
]

urban_trend = [
    urban_cover - 200,
    urban_cover - 100,
    urban_cover - 50,
    urban_cover - 20,
    urban_cover
]

import numpy as np
from sklearn.linear_model import LinearRegression

years_np = np.array(years).reshape(-1,1)

green_model = LinearRegression().fit(years_np, green_trend)
urban_model = LinearRegression().fit(years_np, urban_trend)

future_year = np.array([[2025]])

green_2025 = green_model.predict(future_year)[0]
urban_2025 = urban_model.predict(future_year)[0]

print("Predicted Green Cover 2025:", round(green_2025))
print("Predicted Urban Land 2025:", round(urban_2025))

future_years = years + [2025]
future_green = green_trend + [green_2025]
future_urban = urban_trend + [urban_2025]

import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.plot(future_years, future_green, marker='o', label="Green Cover")
plt.plot(future_years, future_urban, marker='o', label="Urban")

plt.axvline(x=2022, linestyle='--')  # separates real vs predicted

plt.title("Land Sustainability Forecast to 2025")
plt.xlabel("Year")
plt.ylabel("Land Count")
plt.legend()
plt.show()